#######################################################
#  deberta_param
#######################################################
globals:
  debug: False
  seed: 2023

competition_name: CommonLit2023
experiment_name: default
model_name: microsoft/deberta-v3-base
pooling: mean
reinit_layers: 0
gradient_checkpointing: True
apex: True
output_dir: outputs
trn_fold: [0, 1, 2, 3, 4]
epochs: 4
print_freq: 20
gradient_accumulation_steps: 1
max_grad_norm: 1000
batch_scheduler: True

split:
  name: Content_StratifiedKFold
  params:
    n_splits: 5
    shuffle: True
    random_state: 2023

dataset:
  name: CustomDataset
  params:
    max_len: 822

loader:
  train:
    batch_size: 8
    shuffle: True
    num_workers: 4
    pin_memory: True
    drop_last: True
  valid:
    batch_size: 16
    shuffle: False
    num_workers: 4
    pin_memory: True
    drop_last: False

loss:
  name: SmoothL1Loss

optimizer:
  name: AdamW
  params:
    lr: 2e-5
    weight_decay: 0.01
    betas_min: 0.9
    betas_max: 0.999
    eps: 1e-6

scheduler:
  name: get_cosine_schedule_with_warmup
  params:
    num_warmup_steps: 0
    T_max: 10
    eta_min: 1e-5
    num_cycles: 0.5

